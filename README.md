
# Music Data Processing Pipeline

An end-to-end ETL pipeline that processes song, user, and stream data using AWS Glue and Apache Airflow (via Amazon MWAA), performing data validation, transformation, and loading into DynamoDB, with automated alerts and S3 archival.


---

## Overview

This pipeline automates the extraction, validation, transformation, and loading (ETL) of music streaming data. It is orchestrated with Apache Airflow on Amazon MWAA, and leverages AWS Glue jobs for batch processing and validation.

---

## Architecture

```
S3 (raw data)
   â”‚
   â””â”€â”€â–¶ S3KeySensor (Airflow)
         â”‚
         â””â”€â”€â–¶ run_validation_job (Glue: validation checks)
                 â”‚
                 â””â”€â”€â–¶ check_validation_status
                           â”œâ”€â”€â–¶ Success â†’ run_transform_job (Glue)
                           â””â”€â”€â–¶ Failure â†’ notify_failure (SNS)
                                     â”‚
                                     â†“
                               check_transform_status
                                     â”‚
                                     â”œâ”€â”€â–¶ Success â†’ load_to_dynamodb
                                     â”‚                â†“
                                     â”‚          archive_streamed_data
                                     â”‚                â†“
                                     â”‚          notify_success
                                     â””â”€â”€â–¶ Failure â†’ notify_failure
```

---

## Features

- **Schema & Null Validation** using PySpark in Glue jobs  
- **Bad Data Handling**: Invalid rows routed to separate S3 path  
- **Foreign Key Integrity** checks across songs, users, and streams  
- **Glue Transform Jobs** for cleaning & enrichment  
- **Airflow DAG** with branching logic and retries  
- **SNS Notification** on success/failure  
- **S3 Archival** of streamed files  

---

## Technologies

- **AWS S3**: Raw & Processed data storage  
- **AWS Glue**: Data processing and cleaning jobs  
- **AWS DynamoDB**: Stores KPI outputs  
- **Apache Airflow** (Amazon MWAA): Workflow orchestration  
- **Amazon SNS**: Success/Failure notifications  

---

## Setup Instructions

### Prerequisites

- AWS account with IAM roles set for:
  - Glue
  - S3
  - DynamoDB
  - SNS
- Amazon MWAA environment configured
- Airflow connection named `aws_default`

### Environment Variables (Airflow)

Set the following in Airflow Variables:

| Variable | Description |
|----------|-------------|
| `GLUE_JOB_NAME` | Name of the transformation Glue job |
| `DYNAMODB_GLUE_JOB_NAME` | Glue job for loading data to DynamoDB |
| `S3_BUCKET` | Main S3 bucket name |
| `S3_STREAMING_PREFIX` | S3 prefix for incoming streams |
| `SNS_TOPIC_ARN` | ARN of your SNS topic |

---

## DAG Flow Summary

### 1. Wait for Stream Files
Airflow watches S3 for new files under the defined streaming prefix.
Detect `.csv` files under `s3://<bucket>/raw_data/streams/`

### 2. Run Validation Glue Job
Checks schema, nulls, duplicates, ranges, and referential integrity

### 3. If Validation Passes
â†’ Runs transformation Glue job to clean & process data

### 4. Load to DynamoDB
â†’ Runs another Glue job to load results to 3 DynamoDB tables:
- `genre_kpis`
- `top_songs`
- `top_genres`

### 5. Archive Streamed Files
Moves used files to `archived/<date>/` folder in S3

### 6. Notifications
Sends success/failure message via Amazon SNS

---

## Glue Transform Functions

Each Glue transform uses a custom function that outputs:

```python
DynamicFrameCollection({
    "clean": clean_dyf,
    "bad": bad_dyf
}, glueContext)
```

- **`MyTransform_songs`**: Cleans song metadata  
- **`MyTransform_users`**: Validates user data and age range  
- **`MyTransform_streams`**: Ensures stream event completeness

---
## KPI Tables and Insights
1. genre_kpi â€“ Genre-Level Engagement Summary

| Column            | Description                                      |
| ----------------- | ------------------------------------------------ |
| `genre`           | Music genre name (e.g., Pop, Rock)               |
| `date`            | Date of aggregation                              |
| `total_streams`   | Total stream count for this genre                |
| `unique_users`    | Number of distinct users who streamed this genre |
| `avg_listen_time` | Average listen duration (seconds)                |

### Insights:
- Spot genre trends across time (e.g., weekly rise in Afrobeat)
- Evaluate genre stickiness via average listen time
- Determine engagement patterns by unique users per genre


2. top_songs â€“ Daily Ranked Songs by Genre

| Column         | Description                            |
| -------------- | -------------------------------------- |
| `date`         | Aggregation date                       |
| `genre`        | Song's genre                           |
| `track_id`     | Unique ID of the track                 |
| `title`        | Name of the song                       |
| `artist`       | Performing artist                      |
| `stream_count` | Number of streams recorded on that day |
| `rank`         | Rank based on descending stream count  |
| `date#genre`   | Composite key for DynamoDB             |

### Insights:
- Identify daily or weekly top-performing songs
- Track artist momentum over time
- Analyze which genres are contributing the most top tracks


3. top_genres â€“ Ranked Genre Summary by Day

| Column          | Description                                   |
| --------------- | --------------------------------------------- |
| `date`          | Date of aggregation                           |
| `genre`         | Genre name                                    |
| `total_streams` | Total stream count for this genre on the date |
| `rank`          | Rank based on total daily stream count        |

### Insights:
- High-level genre trend monitoring
- Real-time performance of music categories
- Marketing insight for genre-focused campaigns
---

## ðŸ“‚ Folder Structure

```
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ streaming_pipeline_dag.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ validate_job.py
â”‚   â”œâ”€â”€ transform_job.py
â”‚   â””â”€â”€ load_to_dynamodb.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ songs/
â”‚   â”œâ”€â”€ streams/
â”‚   â””â”€â”€ users/
â”œâ”€â”€ processed_data/
â”‚   â”œâ”€â”€ genre_kpis/
â”‚   â”œâ”€â”€ top_genres/
â”‚   â””â”€â”€ top_songs/
â”œâ”€â”€ bad_rows/
â”‚   â”œâ”€â”€ songs_bad_rows.csv/
â”‚   â”œâ”€â”€ streams_bad_rows.csv/
â”‚   â””â”€â”€ users_bad_rows.csv/
â”œâ”€â”€ architecture.png
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```
---
